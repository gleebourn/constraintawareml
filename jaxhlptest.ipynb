{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79870a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path,argv\n",
    "from pathlib import Path\n",
    "path.append(str(Path('.').absolute()))\n",
    "\n",
    "from pandas import read_csv\n",
    "from jax.numpy import array\n",
    "\n",
    "from cal.jaxn import hidden_layer_perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003b49f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape= (175341, 40)\n",
      "y.shape= (175341,)\n",
      "X= [[1.000000e+00 1.214780e-01 6.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  0.000000e+00]\n",
      " [2.000000e+00 6.499020e-01 1.400000e+01 ... 1.000000e+00 6.000000e+00\n",
      "  0.000000e+00]\n",
      " [3.000000e+00 1.623129e+00 8.000000e+00 ... 2.000000e+00 6.000000e+00\n",
      "  0.000000e+00]\n",
      " ...\n",
      " [1.753390e+05 9.000000e-06 2.000000e+00 ... 3.000000e+00 1.200000e+01\n",
      "  0.000000e+00]\n",
      " [1.753400e+05 9.000000e-06 2.000000e+00 ... 3.000000e+01 3.000000e+01\n",
      "  0.000000e+00]\n",
      " [1.753410e+05 9.000000e-06 2.000000e+00 ... 3.000000e+01 3.000000e+01\n",
      "  0.000000e+00]]\n",
      "y= [0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def get_Xy(filename):\n",
    "  Xy=read_csv(filename).select_dtypes(include='number')\n",
    "  return array(Xy.drop(columns='label'),dtype=float),array(Xy['label'],dtype=float)\n",
    "\n",
    "X,y=get_Xy('~/data/UNSW_NB15_training-set.csv')\n",
    "\n",
    "print('X.shape=',X.shape)\n",
    "print('y.shape=',y.shape)\n",
    "print('X=',X)\n",
    "print('y=',y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e463da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp_gd=hidden_layer_perceptron(X.shape[1],32,lr=.01)\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "n_epochs=50\n",
    "#Just randomly sample with replacement, after a few epochs each point samples ~#epochs times anyway\n",
    "batches_per_epoch=int(len(y)/batch_size)\n",
    "n_batches=batches_per_epoch*n_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4552ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Batches per epoch: 2739\n",
      "Epochs to run: 50\n"
     ]
    }
   ],
   "source": [
    "print('Batch size:',batch_size)\n",
    "print('Batches per epoch:',batches_per_epoch)\n",
    "print('Epochs to run:',n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbc7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,y_test=get_Xy('~/data/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46123b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting \"epoch\" 1 ...\n",
      "...done!\n",
      "\"Epoch\" 1 whole training set loss: 0.2353319\n",
      "\"Epoch\" 1 whole test set loss: 0.38279954\n",
      "Starting \"epoch\" 2 ...\n",
      "...done!\n",
      "\"Epoch\" 2 whole training set loss: 0.2239568\n",
      "\"Epoch\" 2 whole test set loss: 0.37418896\n",
      "Starting \"epoch\" 3 ...\n",
      "...done!\n",
      "\"Epoch\" 3 whole training set loss: 0.21854514\n",
      "\"Epoch\" 3 whole test set loss: 0.3689194\n",
      "Starting \"epoch\" 4 ...\n",
      "...done!\n",
      "\"Epoch\" 4 whole training set loss: 0.21361893\n",
      "\"Epoch\" 4 whole test set loss: 0.3651313\n",
      "Starting \"epoch\" 5 ...\n",
      "...done!\n",
      "\"Epoch\" 5 whole training set loss: 0.20988259\n",
      "\"Epoch\" 5 whole test set loss: 0.36248305\n",
      "Starting \"epoch\" 6 ...\n",
      "...done!\n",
      "\"Epoch\" 6 whole training set loss: 0.20737366\n",
      "\"Epoch\" 6 whole test set loss: 0.36061385\n",
      "Starting \"epoch\" 7 ...\n",
      "...done!\n",
      "\"Epoch\" 7 whole training set loss: 0.20565364\n",
      "\"Epoch\" 7 whole test set loss: 0.3592518\n",
      "Starting \"epoch\" 8 ...\n",
      "...done!\n",
      "\"Epoch\" 8 whole training set loss: 0.20441565\n",
      "\"Epoch\" 8 whole test set loss: 0.35820308\n",
      "Starting \"epoch\" 9 ...\n",
      "...done!\n",
      "\"Epoch\" 9 whole training set loss: 0.20348465\n",
      "\"Epoch\" 9 whole test set loss: 0.35734054\n",
      "Starting \"epoch\" 10 ...\n",
      "...done!\n",
      "\"Epoch\" 10 whole training set loss: 0.2027594\n",
      "\"Epoch\" 10 whole test set loss: 0.35657862\n",
      "Starting \"epoch\" 11 ...\n",
      "...done!\n",
      "\"Epoch\" 11 whole training set loss: 0.20217872\n",
      "\"Epoch\" 11 whole test set loss: 0.35585222\n",
      "Starting \"epoch\" 12 ...\n",
      "...done!\n",
      "\"Epoch\" 12 whole training set loss: 0.20170501\n",
      "\"Epoch\" 12 whole test set loss: 0.3550998\n",
      "Starting \"epoch\" 13 ...\n",
      "...done!\n",
      "\"Epoch\" 13 whole training set loss: 0.20131825\n",
      "\"Epoch\" 13 whole test set loss: 0.35424972\n",
      "Starting \"epoch\" 14 ...\n",
      "...done!\n",
      "\"Epoch\" 14 whole training set loss: 0.20101815\n",
      "\"Epoch\" 14 whole test set loss: 0.35319835\n",
      "Starting \"epoch\" 15 ...\n",
      "...done!\n",
      "\"Epoch\" 15 whole training set loss: 0.20084432\n",
      "\"Epoch\" 15 whole test set loss: 0.35178524\n",
      "Starting \"epoch\" 16 ...\n",
      "...done!\n",
      "\"Epoch\" 16 whole training set loss: 0.20093027\n",
      "\"Epoch\" 16 whole test set loss: 0.34982407\n",
      "Starting \"epoch\" 17 ...\n",
      "...done!\n",
      "\"Epoch\" 17 whole training set loss: 0.20146364\n",
      "\"Epoch\" 17 whole test set loss: 0.34745654\n",
      "Starting \"epoch\" 18 ...\n",
      "...done!\n",
      "\"Epoch\" 18 whole training set loss: 0.20216252\n",
      "\"Epoch\" 18 whole test set loss: 0.3454854\n",
      "Starting \"epoch\" 19 ...\n",
      "...done!\n",
      "\"Epoch\" 19 whole training set loss: 0.20250429\n",
      "\"Epoch\" 19 whole test set loss: 0.34418374\n",
      "Starting \"epoch\" 20 ...\n",
      "...done!\n",
      "\"Epoch\" 20 whole training set loss: 0.20244655\n",
      "\"Epoch\" 20 whole test set loss: 0.3431714\n",
      "Starting \"epoch\" 21 ...\n",
      "...done!\n",
      "\"Epoch\" 21 whole training set loss: 0.20209432\n",
      "\"Epoch\" 21 whole test set loss: 0.34219554\n",
      "Starting \"epoch\" 22 ...\n",
      "...done!\n",
      "\"Epoch\" 22 whole training set loss: 0.20153518\n",
      "\"Epoch\" 22 whole test set loss: 0.3411773\n",
      "Starting \"epoch\" 23 ...\n",
      "...done!\n",
      "\"Epoch\" 23 whole training set loss: 0.20083553\n",
      "\"Epoch\" 23 whole test set loss: 0.34011313\n",
      "Starting \"epoch\" 24 ...\n",
      "...done!\n",
      "\"Epoch\" 24 whole training set loss: 0.200046\n",
      "\"Epoch\" 24 whole test set loss: 0.33902004\n",
      "Starting \"epoch\" 25 ...\n",
      "...done!\n",
      "\"Epoch\" 25 whole training set loss: 0.1992059\n",
      "\"Epoch\" 25 whole test set loss: 0.33792192\n",
      "Starting \"epoch\" 26 ...\n",
      "...done!\n",
      "\"Epoch\" 26 whole training set loss: 0.19834293\n",
      "\"Epoch\" 26 whole test set loss: 0.33684137\n",
      "Starting \"epoch\" 27 ...\n",
      "...done!\n",
      "\"Epoch\" 27 whole training set loss: 0.19747737\n",
      "\"Epoch\" 27 whole test set loss: 0.33579233\n",
      "Starting \"epoch\" 28 ...\n",
      "...done!\n",
      "\"Epoch\" 28 whole training set loss: 0.1966248\n",
      "\"Epoch\" 28 whole test set loss: 0.33478633\n",
      "Starting \"epoch\" 29 ...\n",
      "...done!\n",
      "\"Epoch\" 29 whole training set loss: 0.19579516\n",
      "\"Epoch\" 29 whole test set loss: 0.33383077\n",
      "Starting \"epoch\" 30 ...\n",
      "...done!\n",
      "\"Epoch\" 30 whole training set loss: 0.19499521\n",
      "\"Epoch\" 30 whole test set loss: 0.3329293\n",
      "Starting \"epoch\" 31 ...\n",
      "...done!\n",
      "\"Epoch\" 31 whole training set loss: 0.19422823\n",
      "\"Epoch\" 31 whole test set loss: 0.33208147\n",
      "Starting \"epoch\" 32 ...\n",
      "...done!\n",
      "\"Epoch\" 32 whole training set loss: 0.1934965\n",
      "\"Epoch\" 32 whole test set loss: 0.3312867\n",
      "Starting \"epoch\" 33 ...\n",
      "...done!\n",
      "\"Epoch\" 33 whole training set loss: 0.19280055\n",
      "\"Epoch\" 33 whole test set loss: 0.33054376\n",
      "Starting \"epoch\" 34 ...\n",
      "...done!\n",
      "\"Epoch\" 34 whole training set loss: 0.19214013\n",
      "\"Epoch\" 34 whole test set loss: 0.3298498\n",
      "Starting \"epoch\" 35 ...\n",
      "...done!\n",
      "\"Epoch\" 35 whole training set loss: 0.19151382\n",
      "\"Epoch\" 35 whole test set loss: 0.32920265\n",
      "Starting \"epoch\" 36 ...\n",
      "...done!\n",
      "\"Epoch\" 36 whole training set loss: 0.19092159\n",
      "\"Epoch\" 36 whole test set loss: 0.3285974\n",
      "Starting \"epoch\" 37 ...\n",
      "...done!\n",
      "\"Epoch\" 37 whole training set loss: 0.19036075\n",
      "\"Epoch\" 37 whole test set loss: 0.32803288\n",
      "Starting \"epoch\" 38 ...\n",
      "...done!\n",
      "\"Epoch\" 38 whole training set loss: 0.18983011\n",
      "\"Epoch\" 38 whole test set loss: 0.32750598\n",
      "Starting \"epoch\" 39 ...\n",
      "...done!\n",
      "\"Epoch\" 39 whole training set loss: 0.18932791\n",
      "\"Epoch\" 39 whole test set loss: 0.3270129\n",
      "Starting \"epoch\" 40 ...\n",
      "...done!\n",
      "\"Epoch\" 40 whole training set loss: 0.18885286\n",
      "\"Epoch\" 40 whole test set loss: 0.32655233\n",
      "Starting \"epoch\" 41 ...\n",
      "...done!\n",
      "\"Epoch\" 41 whole training set loss: 0.18840298\n",
      "\"Epoch\" 41 whole test set loss: 0.3261206\n",
      "Starting \"epoch\" 42 ...\n",
      "...done!\n",
      "\"Epoch\" 42 whole training set loss: 0.18797624\n",
      "\"Epoch\" 42 whole test set loss: 0.32571685\n",
      "Starting \"epoch\" 43 ...\n",
      "...done!\n",
      "\"Epoch\" 43 whole training set loss: 0.18757187\n",
      "\"Epoch\" 43 whole test set loss: 0.32533675\n",
      "Starting \"epoch\" 44 ...\n",
      "...done!\n",
      "\"Epoch\" 44 whole training set loss: 0.18718795\n",
      "\"Epoch\" 44 whole test set loss: 0.32498077\n",
      "Starting \"epoch\" 45 ...\n",
      "...done!\n",
      "\"Epoch\" 45 whole training set loss: 0.18682387\n",
      "\"Epoch\" 45 whole test set loss: 0.32464588\n",
      "Starting \"epoch\" 46 ...\n",
      "...done!\n",
      "\"Epoch\" 46 whole training set loss: 0.18647821\n",
      "\"Epoch\" 46 whole test set loss: 0.3243321\n",
      "Starting \"epoch\" 47 ...\n",
      "...done!\n",
      "\"Epoch\" 47 whole training set loss: 0.18614946\n",
      "\"Epoch\" 47 whole test set loss: 0.32403517\n",
      "Starting \"epoch\" 48 ...\n",
      "...done!\n",
      "\"Epoch\" 48 whole training set loss: 0.1858361\n",
      "\"Epoch\" 48 whole test set loss: 0.3237542\n",
      "Starting \"epoch\" 49 ...\n",
      "...done!\n",
      "\"Epoch\" 49 whole training set loss: 0.18553868\n",
      "\"Epoch\" 49 whole test set loss: 0.32349172\n",
      "Starting \"epoch\" 50 ...\n",
      "...done!\n",
      "\"Epoch\" 50 whole training set loss: 0.18525448\n",
      "\"Epoch\" 50 whole test set loss: 0.32324308\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,n_epochs+1):\n",
    "  print('Starting \"epoch\"',i,'...')\n",
    "  for j in range(batches_per_epoch):\n",
    "    hlp_gd.gradient_desc_step(X,y,mk_batch=batch_size,noise=0)\n",
    "  print('...done!')\n",
    "  print('\"Epoch\"',i,'whole training set loss:',hlp_gd.loss_batch(X,y))\n",
    "  print('\"Epoch\"',i,'whole test set loss:',hlp_gd.loss_batch(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1bcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp_adam=hidden_layer_perceptron(X.shape[1],32,lr=.01)\n",
    "hlp_adam.init_adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a2590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting \"epoch\" 1 ...\n",
      "...done!\n",
      "\"Epoch\" 1 whole training set loss: 0.20364152\n",
      "\"Epoch\" 1 whole test set loss: 0.35772604\n",
      "Starting \"epoch\" 2 ...\n",
      "...done!\n",
      "\"Epoch\" 2 whole training set loss: 0.18849951\n",
      "\"Epoch\" 2 whole test set loss: 0.32617202\n",
      "Starting \"epoch\" 3 ...\n",
      "...done!\n",
      "\"Epoch\" 3 whole training set loss: 0.17726375\n",
      "\"Epoch\" 3 whole test set loss: 0.31716803\n",
      "Starting \"epoch\" 4 ...\n",
      "...done!\n",
      "\"Epoch\" 4 whole training set loss: 0.17546032\n",
      "\"Epoch\" 4 whole test set loss: 0.31602618\n",
      "Starting \"epoch\" 5 ...\n",
      "...done!\n",
      "\"Epoch\" 5 whole training set loss: 0.17515044\n",
      "\"Epoch\" 5 whole test set loss: 0.3158119\n",
      "Starting \"epoch\" 6 ...\n",
      "...done!\n",
      "\"Epoch\" 6 whole training set loss: 0.17515112\n",
      "\"Epoch\" 6 whole test set loss: 0.31579122\n",
      "Starting \"epoch\" 7 ...\n",
      "...done!\n",
      "\"Epoch\" 7 whole training set loss: 0.17520548\n",
      "\"Epoch\" 7 whole test set loss: 0.31581208\n",
      "Starting \"epoch\" 8 ...\n",
      "...done!\n",
      "\"Epoch\" 8 whole training set loss: 0.17526089\n",
      "\"Epoch\" 8 whole test set loss: 0.31584242\n",
      "Starting \"epoch\" 9 ...\n",
      "...done!\n",
      "\"Epoch\" 9 whole training set loss: 0.17530704\n",
      "\"Epoch\" 9 whole test set loss: 0.31588164\n",
      "Starting \"epoch\" 10 ...\n",
      "...done!\n",
      "\"Epoch\" 10 whole training set loss: 0.17535709\n",
      "\"Epoch\" 10 whole test set loss: 0.31597963\n",
      "Starting \"epoch\" 11 ...\n",
      "...done!\n",
      "\"Epoch\" 11 whole training set loss: 0.1753809\n",
      "\"Epoch\" 11 whole test set loss: 0.3160775\n",
      "Starting \"epoch\" 12 ...\n",
      "...done!\n",
      "\"Epoch\" 12 whole training set loss: 0.17539825\n",
      "\"Epoch\" 12 whole test set loss: 0.31619385\n",
      "Starting \"epoch\" 13 ...\n",
      "...done!\n",
      "\"Epoch\" 13 whole training set loss: 0.17539962\n",
      "\"Epoch\" 13 whole test set loss: 0.31629798\n",
      "Starting \"epoch\" 14 ...\n",
      "...done!\n",
      "\"Epoch\" 14 whole training set loss: 0.17540894\n",
      "\"Epoch\" 14 whole test set loss: 0.3162974\n",
      "Starting \"epoch\" 15 ...\n",
      "...done!\n",
      "\"Epoch\" 15 whole training set loss: 0.17545152\n",
      "\"Epoch\" 15 whole test set loss: 0.3163232\n",
      "Starting \"epoch\" 16 ...\n",
      "...done!\n",
      "\"Epoch\" 16 whole training set loss: 0.17548051\n",
      "\"Epoch\" 16 whole test set loss: 0.31634215\n",
      "Starting \"epoch\" 17 ...\n",
      "...done!\n",
      "\"Epoch\" 17 whole training set loss: 0.17550081\n",
      "\"Epoch\" 17 whole test set loss: 0.31635192\n",
      "Starting \"epoch\" 18 ...\n",
      "...done!\n",
      "\"Epoch\" 18 whole training set loss: 0.17552252\n",
      "\"Epoch\" 18 whole test set loss: 0.3163722\n",
      "Starting \"epoch\" 19 ...\n",
      "...done!\n",
      "\"Epoch\" 19 whole training set loss: 0.17553367\n",
      "\"Epoch\" 19 whole test set loss: 0.31637266\n",
      "Starting \"epoch\" 20 ...\n",
      "...done!\n",
      "\"Epoch\" 20 whole training set loss: 0.17554925\n",
      "\"Epoch\" 20 whole test set loss: 0.316387\n",
      "Starting \"epoch\" 21 ...\n",
      "...done!\n",
      "\"Epoch\" 21 whole training set loss: 0.17556277\n",
      "\"Epoch\" 21 whole test set loss: 0.31638417\n",
      "Starting \"epoch\" 22 ...\n",
      "...done!\n",
      "\"Epoch\" 22 whole training set loss: 0.17557533\n",
      "\"Epoch\" 22 whole test set loss: 0.3163957\n",
      "Starting \"epoch\" 23 ...\n",
      "...done!\n",
      "\"Epoch\" 23 whole training set loss: 0.17558314\n",
      "\"Epoch\" 23 whole test set loss: 0.31640428\n",
      "Starting \"epoch\" 24 ...\n",
      "...done!\n",
      "\"Epoch\" 24 whole training set loss: 0.1755954\n",
      "\"Epoch\" 24 whole test set loss: 0.31641346\n",
      "Starting \"epoch\" 25 ...\n",
      "...done!\n",
      "\"Epoch\" 25 whole training set loss: 0.1755895\n",
      "\"Epoch\" 25 whole test set loss: 0.31640327\n",
      "Starting \"epoch\" 26 ...\n",
      "...done!\n",
      "\"Epoch\" 26 whole training set loss: 0.17560391\n",
      "\"Epoch\" 26 whole test set loss: 0.3164126\n",
      "Starting \"epoch\" 27 ...\n",
      "...done!\n",
      "\"Epoch\" 27 whole training set loss: 0.17561485\n",
      "\"Epoch\" 27 whole test set loss: 0.31642038\n",
      "Starting \"epoch\" 28 ...\n",
      "...done!\n",
      "\"Epoch\" 28 whole training set loss: 0.17562282\n",
      "\"Epoch\" 28 whole test set loss: 0.3164275\n",
      "Starting \"epoch\" 29 ...\n",
      "...done!\n",
      "\"Epoch\" 29 whole training set loss: 0.17562865\n",
      "\"Epoch\" 29 whole test set loss: 0.31643236\n",
      "Starting \"epoch\" 30 ...\n",
      "...done!\n",
      "\"Epoch\" 30 whole training set loss: 0.17562704\n",
      "\"Epoch\" 30 whole test set loss: 0.31642157\n",
      "Starting \"epoch\" 31 ...\n",
      "...done!\n",
      "\"Epoch\" 31 whole training set loss: 0.1756247\n",
      "\"Epoch\" 31 whole test set loss: 0.31642443\n",
      "Starting \"epoch\" 32 ...\n",
      "...done!\n",
      "\"Epoch\" 32 whole training set loss: 0.17564078\n",
      "\"Epoch\" 32 whole test set loss: 0.31643352\n",
      "Starting \"epoch\" 33 ...\n",
      "...done!\n",
      "\"Epoch\" 33 whole training set loss: 0.17564474\n",
      "\"Epoch\" 33 whole test set loss: 0.31643686\n",
      "Starting \"epoch\" 34 ...\n",
      "...done!\n",
      "\"Epoch\" 34 whole training set loss: 0.17565076\n",
      "\"Epoch\" 34 whole test set loss: 0.31644338\n",
      "Starting \"epoch\" 35 ...\n",
      "...done!\n",
      "\"Epoch\" 35 whole training set loss: 0.17565341\n",
      "\"Epoch\" 35 whole test set loss: 0.31644595\n",
      "Starting \"epoch\" 36 ...\n",
      "...done!\n",
      "\"Epoch\" 36 whole training set loss: 0.17565268\n",
      "\"Epoch\" 36 whole test set loss: 0.31644902\n",
      "Starting \"epoch\" 37 ...\n",
      "...done!\n",
      "\"Epoch\" 37 whole training set loss: 0.17564945\n",
      "\"Epoch\" 37 whole test set loss: 0.31643364\n",
      "Starting \"epoch\" 38 ...\n",
      "...done!\n",
      "\"Epoch\" 38 whole training set loss: 0.17565249\n",
      "\"Epoch\" 38 whole test set loss: 0.31643796\n",
      "Starting \"epoch\" 39 ...\n",
      "...done!\n",
      "\"Epoch\" 39 whole training set loss: 0.1756671\n",
      "\"Epoch\" 39 whole test set loss: 0.3164466\n",
      "Starting \"epoch\" 40 ...\n",
      "...done!\n",
      "\"Epoch\" 40 whole training set loss: 0.17567001\n",
      "\"Epoch\" 40 whole test set loss: 0.31645095\n",
      "Starting \"epoch\" 41 ...\n",
      "...done!\n",
      "\"Epoch\" 41 whole training set loss: 0.1756733\n",
      "\"Epoch\" 41 whole test set loss: 0.31645334\n",
      "Starting \"epoch\" 42 ...\n",
      "...done!\n",
      "\"Epoch\" 42 whole training set loss: 0.17567334\n",
      "\"Epoch\" 42 whole test set loss: 0.316456\n",
      "Starting \"epoch\" 43 ...\n",
      "...done!\n",
      "\"Epoch\" 43 whole training set loss: 0.17567961\n",
      "\"Epoch\" 43 whole test set loss: 0.31646168\n",
      "Starting \"epoch\" 44 ...\n",
      "...done!\n",
      "\"Epoch\" 44 whole training set loss: 0.17568159\n",
      "\"Epoch\" 44 whole test set loss: 0.31646326\n",
      "Starting \"epoch\" 45 ...\n",
      "...done!\n",
      "\"Epoch\" 45 whole training set loss: 0.17567922\n",
      "\"Epoch\" 45 whole test set loss: 0.31646425\n",
      "Starting \"epoch\" 46 ...\n",
      "...done!\n",
      "\"Epoch\" 46 whole training set loss: 0.17568329\n",
      "\"Epoch\" 46 whole test set loss: 0.3164677\n",
      "Starting \"epoch\" 47 ...\n",
      "...done!\n",
      "\"Epoch\" 47 whole training set loss: 0.17569105\n",
      "\"Epoch\" 47 whole test set loss: 0.31647307\n",
      "Starting \"epoch\" 48 ...\n",
      "...done!\n",
      "\"Epoch\" 48 whole training set loss: 0.17569125\n",
      "\"Epoch\" 48 whole test set loss: 0.31647328\n",
      "Starting \"epoch\" 49 ...\n",
      "...done!\n",
      "\"Epoch\" 49 whole training set loss: 0.17569184\n",
      "\"Epoch\" 49 whole test set loss: 0.316461\n",
      "Starting \"epoch\" 50 ...\n",
      "...done!\n",
      "\"Epoch\" 50 whole training set loss: 0.17569302\n",
      "\"Epoch\" 50 whole test set loss: 0.3164642\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,n_epochs+1):\n",
    "  print('Starting \"epoch\"',i,'...')\n",
    "  for j in range(batches_per_epoch):\n",
    "    hlp_adam.adam_step(X,y,mk_batch=batch_size)\n",
    "  print('...done!')\n",
    "  print('\"Epoch\"',i,'whole training set loss:',hlp_adam.loss_batch(X,y))\n",
    "  print('\"Epoch\"',i,'whole test set loss:',hlp_adam.loss_batch(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07a96b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gleeb",
   "language": "python",
   "name": "gleeb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
