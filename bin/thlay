#!/usr/bin/env python
from pickle import load
from time import perf_counter
from numpy import sum as nsm
from jax.numpy import exp,sin,inf
from jax.random import key,split,choice
from jax.profiler import save_device_memory_profile
from sys import path,stdin
from os import mkdir,environ
#environ["XLA_PYTHON_CLIENT_PREALLOCATE"]="false"
#environ["XLA_PYTHON_CLIENT_MEM_FRACTION"]=".99"
#environ["XLA_PYTHON_CLIENT_ALLOCATOR"]="platform"

from os.path import dirname,abspath
from sys import path
path.append(dirname(dirname(abspath(__file__))))
from cal.thlay import init_ensemble,init_experiments,activations,f_to_str,save_ensemble,\
                      exp_to_str,fpfnp_lab,f,dlosses,get_xy,report_progress,update_lrs,\
                      evaluate_fp_fn,update_history,compute_U_V,update_weights,resnet,\
                      read_input_if_ready

a=init_ensemble()

global_key,report_key=split(key(a.seed))
def emit_key(report=False):
  global global_key,report_key
  new_key,child_key=split(report_key if report else global_key)
  if report:
    report_key=new_key
  else:
    global_key=new_key
  return child_key

try:
  mkdir(a.out_dir)
  new=True
except FileExistsError:
  print('Already seems to be something there... [O]verwrite, [L]oad or [A]bort?')
  ln=stdin.readline()[0].lower()
  if ln[0]=='l':
    new=False
  elif ln[0]=='o':
    new=True
    print('Overwriting any existing ensembles...')
  else:
    print('Abort!')
    exit()

if not new:
  try:
    with open(a.out_dir+'/ensemble.pkl','rb') as fd:
      print('Opening experiment ensemble',a.outf,'...')
      od=a.out_dir #Correct the actual directory if opened somewhere else
      a,experiments,global_key=load(fd)
      a.out_dir=od
      new=False
      global_key=a.global_key
      print('Restored',a.outf+'.pkl','from disk')
  except FileNotFoundError:
    print('No pkl in directory...')
    new=True

else:
  print('Generating new experiments...')
  
  experiments=init_experiments(a,global_key)

act=activations[a.act] #Would like to pickle but unable to do so to lambdas

def get_timestep(label):
  global tl
  t=perf_counter()
  try: a.time_avgs[label]+=(1+a.clock_avg_rate)*a.clock_avg_rate*float(t-tl)
  except: a.time_avgs[label]=(1+a.clock_avg_rate)*float(t-tl)
  a.time_avgs[label]*=(1-a.clock_avg_rate)
  tl=t

while True:
  a.step+=1
  if not a.step%10:print(a.step)
  xy=get_xy(a,a.imbalances,a.bs,emit_key())

  a.lr_phase+=a.lr_momentum
  for e in experiments:
    if a.stop_on_target and e.steps_to_target:
      continue
    tl=perf_counter()
    e.step+=1
    x,y_t=xy[float(e.p)]

    e.lr=a.lr*exp(a.lr_amplitude*sin(a.lr_phase))
    e.bs=len(y_t)
    get_timestep('start_loop')
    n_its=e.p_its if a.iterate_minority else 1
    for _ in range(n_its): #when force_batch_cost
      if a.resnet:
        y_p=resnet(e.w_model[0],e.w_model[1],x,act=act).flatten()>0
      else:
        y_p=f(e.w_model[0],e.w_model[1],x,act=act).flatten()>0
      get_timestep('threshold')

      evaluate_fp_fn(e,y_p,y_t)
      get_timestep('evaluate_fp_fn')

      if a.no_U_V:
        e.U=e.V=1
      else:
        e.U,e.V=compute_U_V(e.fp,e.fn,e.target_fp,e.target_fn,
                            1 if a.iterate_minority else e.p,
                            sm=a.softmax_U_V,p_scale=a.p_scale,
                            scale_before_sm=a.scale_before_sm)
      get_timestep('U,V')

      if a.single_layer_upd:
        ind=int(choice(emit_key(),a.resnet-1))
        e.loss_val,(a_upd,b_upd)=dlosses[a.loss](e.w_model[0],e.w_model[1],ind,a.zersq,
                                                 a.zerarr,x,y_t,e.U,e.V,act=act)
        start_zeros=[0.]*(ind)
        end_zeros=[0.]*(a.resnet-ind-2)
        e.loss_val,upd=(start_zeros+[a_upd]+end_zeros,start_zeros+[b_upd]+end_zeros)
      elif a.loss=='distribution_flow_cost':
        e.loss_val,upd=dlosses['distribution_flow_cost'](e.w_model[0],e.w_model[1],
                                                         x,y_t,e.U,e.V,e.w_init,act=act)
      else:
        e.loss_val,upd=dlosses[a.loss](e.w_model[0],e.w_model[1],x,y_t,e.U,e.V,act=act)#,
      e.loss_vals[e.step-1]=float(e.loss_val)

      get_timestep('dloss')
      if a.loss=='distribution_flow_cost':
        avg_loss=e.loss_vals.avg()
        if avg_loss<e.loss_target:
          print('Updating start and end params for loss')
          e.w_init-=e.w_init*e.lr
          e.loss_target=(1-e.lr)*avg_loss
          print('w_init,loss_target:',e.w_init,e.loss_target)

      update_weights(a,e,upd)
      get_timestep('update_weights')

    update_history(e)
    get_timestep('update_history')
    
  if not a.step%a.lr_update_interval and a.mode=='adaptive_lr':
    experiments=update_lrs(a,[e for e in experiments if (not a.stop_on_target) and\
                                                        (not e.steps_to_target)],
                           emit_key())

  line=read_input_if_ready()

  if not a.step%a.saving_interval:
    save_ensemble(a,experiments,global_key)

  if line or not a.step%a.reporting_interval:
    report_progress(a,experiments,line,act,emit_key(report=True))
