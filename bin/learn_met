#!/usr/bin/env python
from os.path import dirname,abspath
from sys import path
path.append(dirname(dirname(abspath(__file__))))
from cal.thlay import f,init_layers,rbd24,min_dist,f_to_str,resnet,nn_cost_expansion,\
                      dnn_cost_expansion,update_weights,read_input_if_ready,cyc

from pickle import dump

from types import SimpleNamespace

from scipy.spatial import KDTree

from matplotlib.pyplot import hist,plot,title,legend,show

from jax.numpy import zeros
from jax.nn import tanh,softmax
from jax.random import key,split,permutation#,uniform
from numpy import unique,var,arange,array as nparr,concatenate,array_split,exp,\
                  min as nmn,sum as nsm,round as rnd


(x_train,y_train),(x_test,y_test),(df,x_cols)=rbd24()
n_train=len(y_train)
n_test=len(y_test)
n_train_p=sum(y_train)
n_test_p=sum(y_test)
n_train_n=n_train-n_train_p
n_test_n=n_test-n_train_p
p_train=n_train_p/n_train
p_test=n_test_p/n_test
print('Number of training rows:',n_train)
print('Number of testing rows:',n_test)
print('p_train,p_test:',p_train,',',p_test)

x_train_p=nparr(x_train[y_train])
x_train_n=nparr(x_train[~y_train])
#x_train_p_unique=unique(x_train_p)
#x_train_n_unique=unique(x_train_n)

#md_p=min_dist(x_train_p_unique)
#print('min dist +:',md_p[0])
#print('at points:')
#print(md_p[1])
#print(md_p[2])
#md_n=min_dist(x_train_n_unique)
#print('min dist -:',md_n[0])
#print('at points:')
#print(md_n[1])
#print(md_n[2])
#md_pn=min_dist(x_train_p_unique,x_train_n_unique)
#print('min dist between + and -:',md_pn[0])
#print('at points:')
#print(md_pn[1])
#print(md_pn[2])
#print('len(x_train)-len(unique(x_train)):',len(x_train)-len(unique(x_train)))
#var_vars=list(zip(x_cols,var(x_train,axis=1)))
#var_vars.sort(key=lambda x:x[1])
#[print('V(',c.ljust(40),'):',f_to_str(x)) for c,x in var_vars]

#for i,x in enumerate(x_train):
#  min_feat_dist=min(min_feat_dist,nmn(nsm(((x_train[i:]-x.T)**2),axis=1)))

#def nn_pca(key,x_train,n_layers,lr,n_steps,bs):
#  k0,k1,k2=split(key,3)
#  w_c,b_c=init_layers(k0,[len(x_train[0])]*(n_layers+1),glorot_uniform=True)
#  w_e,b_e=init_layers(k1,[len(x_train[0])]*(n_layers+1),glorot_uniform=True)
#  for i in range(n_steps):

e=SimpleNamespace(w_model=([],[]),w_l2=0.,dw_l2=0.,adam_V=([],[]),target_dist_prop=.1,
                  adam_M=0.,lr=1e-2,min_dist_tolerance=1e-8,target_dist=.1)
a=SimpleNamespace(adam=True,gamma1=.1,gamma2=.001,step=0,lr=1e-2,target_n_layers=10,
                  bs=128,subpoch_num=0,memlen=100,target_separation=10)
n_pos=len(x_train_p)
width=x_train_p.shape[1]

prediction_block_size=10000
n_splits=n_pos//prediction_block_size
k0=key(1729)
#Idea: Speed things up by choosing a smaller sample from which to pick close pts
#Want to check the smallest target_dist_prop distances
subpoch_fraction=int((e.target_dist_prop*n_train_p*n_train_n/(n_train_p+n_train_n))**.5)
n_train_n_subpoch=n_train_n//subpoch_fraction
n_train_p_subpoch=n_train_p//subpoch_fraction #no change in training class imbalance
subpoch_offset_p_next=n_train
subpoch_offset_n_next=n_train
while True:
  k0,k1=split(k0)
  print('Adding a new layer...')
  e.w_model[0].append(zeros((width,width)))
  e.w_model[1].append(zeros((width,)))
  e.adam_V[0].append(zeros((width,width)))
  e.adam_V[1].append(zeros((width,)))
  if subpoch_offset_p_next>n_train_p:
    x_train_p=permutation(k0,x_train_p)
    subpoch_offset_p=0
    subpoch_offset_p_next=n_train_p_subpoch
  if subpoch_offset_n_next>n_train_n:
    x_train_n=permutation(k1,x_train_n)
    subpoch_offset_n=0
    subpoch_offset_n_next=n_train_n_subpoch
  x_train_p_selected=x_train_p[subpoch_offset_p:subpoch_offset_p_next]
  x_train_n_selected=x_train_n[subpoch_offset_n:subpoch_offset_n_next]

  for starting_layer in reversed(range(len(e.w_model[0]))):
    target_exp=a.target_separation
    target_exp**=((len(e.w_model[0])-starting_layer)/(2*a.target_n_layers))
    target_cont=1/target_exp
    z_train_p=resnet(e.w_model[0][:starting_layer],e.w_model[1][:starting_layer],
                     x_train_p_selected)
    z_train_n=resnet(e.w_model[0][:starting_layer],e.w_model[1][:starting_layer],
                     x_train_n_selected)

    print('Generating trees...')
    e.kdtp=KDTree(z_train_p)
    e.kdtn=KDTree(z_train_n)
    print('...complete')

    while True:
      print('Finding distances under',e.target_dist,'between opposite classes...')
      ijv=e.kdtp.sparse_distance_matrix(e.kdtn,e.target_dist,output_type='ndarray')
      lijv=len(ijv)
      print('Found',lijv,'pairs...')
      if(lijv>=n_train//2):
        print('Taking at most n_train of them.')
        ijv=ijv[ijv['v'].argsort()][:n_train]
        n_rows_subpoch=len(ijv)
        print('Next time: target distance',e.target_dist)
        break
      e.target_dist*=min(2,max(.75,n_train/(1+lijv)))
      print('Too few pairs found, less than half the target (',n_train,
            ')!  Trying again with target distance',e.target_dist)
    n_train_subpoch=len(ijv)
    p_subpoch=z_train_p[ijv['i']]
    n_subpoch=z_train_n[ijv['j']]
    dists_subpoch=ijv['v']
    print('Finding nearest neighbours within classes...')
    d_p_nn,i_p_nn=e.kdtp.query(p_subpoch,k=[2],workers=-1)
    d_n_nn,i_n_nn=e.kdtn.query(n_subpoch,k=[2],workers=-1)
    p_nn=z_train_p[i_p_nn].reshape(p_subpoch.shape)
    n_nn=z_train_n[i_n_nn].reshape(p_subpoch.shape)
    if read_input_if_ready():
      hist(dists_subpoch,bins=50,label='minimal distances between classes')
      hist(d_p_nn,bins=50,label='minimal distances within + class')
      hist(d_n_nn,bins=50,label='minimal distances within - class')
      legend()
      title('Distributions of nearest neighbours within '+str(e.target_dist)+\
            ' of the other class after '+str(starting_layer)+' layers')
      show()
    print('Starting subpoch...')
    a.subpoch_num+=1
    #Iterate the algorithm for positive and negative classes
    offset=0
    next_offset=a.bs
    subpoch_step=0
    for a_subpoch,b_subpoch,nn in [(p_subpoch,n_subpoch,p_nn),
                                   (n_subpoch,p_subpoch,n_nn)]:
      #Want to expand away from other class and contract within the class.
      costs_exp=cyc(a.memlen)
      costs_cont=cyc(a.memlen)
      while next_offset<n_rows_subpoch:
        a_batch=a_subpoch[offset:next_offset]
        if dists_subpoch[offset]<e.min_dist_tolerance:
          print('Skipping very small distance (',dists_subpoch[offset],
                '<',e.min_dist_tolerance,')...')
          continue
        c_exp,upd_exp=dnn_cost_expansion(e.w_model[0][starting_layer:],
                                     e.w_model[1][starting_layer:],
                                     a_batch,b_subpoch[offset:next_offset],imp=resnet)
        costs_exp[subpoch_step]=-c_exp

        c_cont,upd_cont=dnn_cost_expansion(e.w_model[0][starting_layer:],
                                           e.w_model[1][starting_layer:],
                                           a_batch,nn[offset:next_offset],
                                           contraction=True,imp=resnet)
        costs_cont[subpoch_step]=c_cont
        avg_cont=costs_cont.avg()/a.bs
        avg_exp=costs_exp.avg()/a.bs
        cont_score=exp(avg_cont/target_cont-1)
        exp_score=exp(1-target_exp/avg_exp)
        if cont_score<1 and exp_score<1:
          break
        s,t=softmax(nparr([cont_score,exp_score]))
        upd=([s*ac+t*ae for ac,ae in zip(upd_cont[0],upd_exp[0])],
             [s*bc+t*be for bc,be in zip(upd_cont[1],upd_exp[1])])
        update_weights(a,e,upd,start=starting_layer)
        if not a.step%100:
          print(a.step)
          print('ms contraction',avg_cont,'update weighting:',s)
          print('ms expansion',avg_exp,'update weighting:',t)
          print('target contraction',target_cont)
          print('target expansion',target_exp)
          print('Sum of squares of model weights:',
                sum([float(nsm(a**2)+nsm(b**2)) for a,b in\
                     zip(e.w_model[0],e.w_model[1])]))
          if not a.step%1000:
            with open('metlearn.pkl','wb') as fd: dump((e,a),fd)
        a.step+=1
        subpoch_step+=1
        offset=next_offset
        next_offset+=a.bs

  
  subpoch_offset_p=subpoch_offset_p_next
  subpoch_offset_n=subpoch_offset_n_next
  subpoch_offset_p_next+=n_train_p_subpoch
  subpoch_offset_n_next+=n_train_n_subpoch

