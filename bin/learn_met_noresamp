#!/usr/bin/env python
from os.path import dirname,abspath
from sys import path
path.append(dirname(dirname(abspath(__file__))))
from cal.thlay import f,init_layers,rbd24,min_dist,f_to_str,resnet,\
                      nn_cost_expansion,dnn_cost_expansion,update_weights,\
                      read_input_if_ready,cyc,dmetric_cost

from pickle import dump

from types import SimpleNamespace

from scipy.spatial import KDTree

from matplotlib.pyplot import hist,plot,title,legend,show

from jax.numpy import zeros,expand_dims,concatenate
from jax.nn import tanh,softmax
from jax.random import key,split,permutation,choice#,uniform
from numpy import unique,var,arange,array as nparr,array_split,exp,log10,\
                  min as nmn,sum as nsm,round as rnd


(x_train,y_train),(x_test,y_test),(df,x_cols)=rbd24()
n_train=len(y_train)
n_test=len(y_test)
n_train_p=sum(y_train)
n_test_p=sum(y_test)
n_train_n=n_train-n_train_p
n_test_n=n_test-n_train_p
p_train=n_train_p/n_train
p_test=n_test_p/n_test
print('Number of training rows:',n_train)
print('Number of testing rows:',n_test)
print('p_train,p_test:',p_train,',',p_test)

e=SimpleNamespace(w_model=([],[]),w_l2=0.,dw_l2=0.,adam_V=([],[]),target_dist_prop=.1,
                  adam_M=0.,lr=1e-2,min_dist_tol=1e-8,target_dist=.1)
a=SimpleNamespace(adam=True,gamma1=.1,gamma2=.001,step=0,lr=1e-2,target_n_layers=50,
                  bs=128,subpoch_num=0,memlen=100,target_separation=10)

width=x_train.shape[1]

prediction_block_size=10000
n_splits=n_test//prediction_block_size
k0=key(1729)
n_layers=100
e.w_model=([zeros((width,width)) for i in range(n_layers)],
           [zeros((width,)) for i in range(n_layers)])
e.adam_V=([zeros((width,width)) for i in range(n_layers)],
          [zeros((width,)) for i in range(n_layers)])
imp=resnet
a.step=0
a.epoch_num=0
a.avg_weight=.2
while True:
  a.epoch_num+=1 #Note this is a crazy long "epoch"!
  k0,k1,k2,k3=split(k0,4)
  perm=permutation(k0,len(y_test))
  x_train,y_train=x_train[perm],y_train[perm]
  cum_change=2
  print('Complete!')
  for i in perm:
    if cum_change>1:
      print('Weights have changed by >1! Recalculating distances')
      cum_change=0.
      z_train=concatenate([imp(*e.w_model,spl) for\
                              spl in array_split(x_train,n_splits)])
      print('Generating new kdtree...')
      kdt=KDTree(z_train)
    x,z=x_train[i],z_train[i]
    a.step+=1
    print('Finding batch by query...')
    dists,inds=kdt.query(z,k=range(1,1+a.bs))
    print('...done!')
    x_b,z_b,y_b=x_train[inds],z_train[inds],y_train[inds]
    num_pos=nsm(y_b)
    if num_pos==0 or num_pos==a.bs:
      print('Skipping point far from other class')
      continue
    zsqs=nsm(z_b**2,axis=1)
    dists_pred=(zsqs+expand_dims(zsqs,-1)-2*zsqs@zsqs.T)**.5
    dist_ideal=y_b^expand_dims(y_b,-1)
    dist_target=a.avg_weight*dist_ideal+(1-a.avg_weight)*dists_pred
    c,upd=dmetric_cost(*e.w_model,x_b,None,imp=resnet,target_dists=dist_target)
    update_weights(a,e,upd)
    cum_change+=e.dw_l2
    if not a.step%10:
      print(a.step)
      if not a.step%1000:
        with open('metlearnnrs.pkl','wb') as fd: dump((e,a),fd)
      if read_input_if_ready():
        c_vis=choice(k1,n_test,(10000,))
        x_vis,y_vis=x_test[c_vis],y_test[c_vis]
        x_vis_p,x_vis_n=x_vis[y_vis],x_vis[~y_vis]
        z_vis_p,z_vis_n=imp(*e.w_model,x_vis_p),imp(*e.w_model,x_vis_n)
        x_sqs_p=jsm(x_vis_p**2,axis=1)
        x_sqs_n=jsm(x_vis_n**2,axis=1)
        z_sqs_p=jsm(x_vis_p**2,axis=1)
        z_sqs_n=jsm(x_vis_n**2,axis=1)
        x_dists_pp=x_sqs_p+expand_dims(x_sqs_p,-1)-2*x_vis_p@x_vis_p.T
        x_dists_nn=x_sqs_n+expand_dims(x_sqs_n,-1)-2*x_vis_n@x_vis_n.T
        z_dists_pp=z_sqs_p+expand_dims(z_sqs_p,-1)-2*z_vis_p@z_vis_p.T
        z_dists_nn=z_sqs_n+expand_dims(z_sqs_n,-1)-2*z_vis_n@z_vis_n.T
        x_dists_pn=jsm((expand_dims(x_vis_p,0)-expand_dims(x_vis_n,1))**2,axis=-1)
        z_dists_pn=jsm((expand_dims(z_vis_p,0)-expand_dims(z_vis_n,1))**2,axis=-1)
        dil_pp=log(z_dists_pp/(1e-8+x_dists_pp))
        dil_pn=log(z_dists_pn/(1e-8+x_dists_pn))
        dil_nn=log(z_dists_nn/(1e-8+x_dists_nn))
        hist(dil_pn,bins=50,label='dist changes between classes',alpha=.4)
        hist(dil_pp,bins=50,label='dist changes within + class',alpha=.4)
        hist(dil_nn,bins=50,label='dist changes within - class',alpha=.4)
        legend()
        title('Distributions of nearest neighbours within '+str(e.target_dist)+\
              ' of the other class after '+str(starting_layer)+' layers')
        show()

