#!/usr/bin/env python
from pickle import dump
from numpy import geomspace
from jax.numpy import inf,array,array_split,zeros,sum as jsm,\
                      exp,zeros,ones,asarray,maximum,minimum,log
from jax.lax import dot_general,scan
from jax.random import key,split,normal
from jax import jit,value_and_grad,grad,vmap
from jaxlib.xla_extension import XlaRuntimeError
from jax.tree import map as jma,reduce as jrd
from jax.nn import tanh
from sys import path,stdin
from os import mkdir,environ,get_terminal_size
from itertools import count
from pathlib import Path
from collections import namedtuple
from os.path import dirname,abspath
from sys import path
path.append(dirname(dirname(abspath(__file__))))
from cal.thlay import activations,f_to_str,shuffle_xy,init_layers,upd_adam,show_unique_stats,\
                      mk_epochs,mk_exps,rbd24,report_epochs,plot_epochs,update_epoch_history,\
                      ewma,mk_cross_entropy,implementation,TimeStepper,KeyEmitter,l2,mk_l1,\
                      mk_l2,upd_grad,gts,mk_hinge,unsw,mk_l2_svd

act_name='relu'#'tanh'
initialisation='glorot_normal' if act_name=='relu' else 'glorot_uniform'
ts=TimeStepper(clock_avg_rate=.1)
ke=KeyEmitter(1729)
act=activations['relu']
imp=implementation('mlp_do',act)
bs=128
sds='P2P_smartphone' #sds=''

#(X_train,Y_train),(X_test,Y_test),_=unsw()
(X_train,Y_train),(X_test,Y_test),(_,X_columns)=rbd24(rescale_log=True,preproc=True,
                                                      categorical=True,single_dataset=sds)
n_batches=len(Y_train)//bs
p_train=Y_train.mean()
p_test=Y_test.mean()
in_dim=len(X_train[0])
n_starts=4
n_ends=4
target_fpfns=[(.05,.025)]
targ_fpfns=[a/b for a,b in target_fpfns]
start_dims=[256]#[64]#[128]#[512]#[128]
end_dims=[128]#[64]#[256]#[16]
lrs=[1e-3,1e-4]#,1e-4,1e-5]#,1e-5]#[1e-3]#[1e-3,1e-4,1e-5,1e-6]#[1e-6]#[1e-5,1e-6]#,1e-5]
regs=[1e-2]#,1e-3]#,1e-4]#[1e-1,1e-2]#,1e-3]##,1e-2]
depths=[4]#[8]#[4,3,2]
beta2s=[.999]
#ps=[.01]#,.1]#,.1]
#mi=0.#1.#2.
#md=0.#1/3
#pids=[{'p':a,'i':a*mi,'d':a*md} for a in ps for mi,md in [(0,0),(2,1/3),(1,1/3)]]
pids=[{'p':.01,'i':0,'d':0},{'p':.1,'i':0,'d':0}]
#pids=[{'p':a,'i':0,'d':a/3} for a in ps] #"No overshoot" params
#mk_beta=lambda a,b,c:1
exps=mk_exps(targ_fpfns,in_dim,initialisation,p_train,ke.emit_key(),
             tfpfns=target_fpfns,pids=pids,beta2s=beta2s,lrs=lrs,regs=regs,
             start_dims=start_dims,end_dims=end_dims,depths=depths)#,mk_beta=mk_beta)
states=[e['state'] for e in exps]
consts=[e['const'] for e in exps]
shapes=[e['shape'] for e in exps]
print('n experiments:',len(exps))
#loss=mk_cross_entropy_hinge(imp,reg=False)
#loss=mk_hinge(imp)
#loss=mk_cross_entropy(imp,reg=False)
#loss=mk_l2_svd(act=act,k=1)#,reg=False)
#loss=mk_l2(imp=imp,reg=False)#,reg=False)
imp_bench=implementation('mlp_nollact',act)
loss=mk_l2(imp=imp,reg=False)#,reg=False)
print('batches size:',bs)
print('batches per epoch:',n_batches)
epochs=mk_epochs(imp,n_batches,bs,loss,imp_bench=imp_bench)
term_size=gts()
epochs_per_eval=1#5
all_benches=[]
for step in count():
  ts.get_timestep('start')
  states,l2_states,consts,recent_benches=epochs(states,consts,X_train,Y_train,X_test,Y_test,
                                                ke.emit_key(epochs_per_eval))
  all_benches+=recent_benches
  osc_times=[list() for _ in states]
  for i,(bns,bns_l) in enumerate(zip(all_benches,all_benches[1:])):
    for j,(b,bl) in enumerate(zip(bns,bns_l)):
      if b['fpfn_err']*bl['fpfn_err']<0:
        osc_times[j].append(i)
  osc_pds=[[o-ol for o,ol in zip(ot[2:],ot[1:])] for ot in osc_times]
  osc_avgs=[sum(ps)/len(ps) if ps else -1 for ps in osc_pds]
  osc_vars=[(sum([(p-pa)**2 for p in ps])/len(ps))**.5 if ps else -1 for ps,pa in zip(osc_pds,osc_avgs)]
  print('Epoch',step*epochs_per_eval)
  f_to_str(('var_rat','osc_avg','osc_var','kp'),p=True)
  avs=[(v/a if a else 1,a,v,c['pid']['p'],) for a,v,c in zip(osc_avgs,osc_vars,consts)]
  [f_to_str(av,p=True) for av in sorted(avs)]

  print(show_unique_stats([{**s,**b,**c,'p_test':p_test,'shape':sh} for s,c,b,sh in\
                           zip(l2_states,consts,all_benches[-1],
                               ['->'.join([str(s) for s in sh]) for sh in shapes])],
                           trunc=True,by='div',prec=4,term_size=term_size,ts=ts),
          flush=True)
  ts.get_timestep('rep_states')
