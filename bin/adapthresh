#!/usr/bin/env python
from pickle import dump
from numpy import geomspace
from jax.numpy import inf,array,array_split,zeros,sum as jsm,\
                      exp,zeros,ones,asarray,maximum,minimum,log
from jax.lax import dot_general,scan
from jax.random import key,split,normal
from jax import jit,value_and_grad,grad,vmap
from jaxlib.xla_extension import XlaRuntimeError
from jax.tree import map as jma,reduce as jrd
from jax.nn import tanh
from sys import path,stdin
from os import mkdir,environ,get_terminal_size
from itertools import count
from pathlib import Path
from collections import namedtuple
from os.path import dirname,abspath
from sys import path
path.append(dirname(dirname(abspath(__file__))))
from cal.thlay import activations,f_to_str,shuffle_xy,init_layers,upd_adam,show_unique_stats,\
                      mk_epochs,mk_exps,rbd24,report_epochs,plot_epochs,update_epoch_history,\
                      ewma,mk_cross_entropy,implementation,TimeStepper,KeyEmitter,l2,mk_l1,\
                      mk_l2,upd_grad,gts,mk_hinge,unsw,mk_l2_svd_do,mk_cross_entropy,norms_states

act_name='relu'#'tanh'
initialisation='glorot_normal' if act_name=='relu' else 'glorot_uniform'
ts=TimeStepper(clock_avg_rate=.1)
ke=KeyEmitter(1729)
act=activations['relu']
bs=128
sds='P2P_smartphone' #sds=''

(X_train,Y_train),(X_test,Y_test),_,sc=unsw(numeric_only=True,rescale='standard')
#(X_train,Y_train),(X_test,Y_test),(_,X_columns)=rbd24(rescale_log=True,preproc=True,
#                                                      categorical=True,single_dataset=sds)
n_batches=len(Y_train)//bs
p_train=Y_train.mean()
p_test=Y_test.mean()
in_dim=len(X_train[0])
n_starts=4
n_ends=4
target_fpfns=[(.05,.025)]
targ_fpfns=[a/b for a,b in target_fpfns]
start_dims=[64]#[32]#[128]#[256]#[64]#[512]#[128]
end_dims=[32]#[16]#[64]#[128]#[256]#[16]
lrs=[1e-3,1e-4,1e-5,1e-6]#1e-3,1e-4,1e-5]#[1e-6,1e-7]#,1e-4,1e-5]#,1e-5]#[1e-3]
batchnorm=True
regs=[0] if batchnorm else [.1,.01]#0]#[1e-2,1e-3,0]#[1,.1]#,1e-3]#,1e-4]#[1e-1,1e-2]#,1e-3]##,1e-2]
depths=[2]#[8]#[4,3,2]
beta2s=[.999]
#lrps=[{'p':.1,'i':0,'d':0},{'p':.5,'i':0,'d':0},{'p':.1,'i':0,'d':0},{'p':.01,'i':0,'d':0}]
lrps=[.1,.01,.001]
exps=mk_exps(targ_fpfns,in_dim,initialisation,p_train,ke.emit_key(),
             tfpfns=target_fpfns,lrps=lrps,beta2s=beta2s,lrs=lrs,regs=regs,
             start_dims=start_dims,end_dims=end_dims,depths=depths)#,mk_beta=mk_beta)
states=[e['state'] for e in exps]
consts=[e['const'] for e in exps]
shapes=[e['shape'] for e in exps]
print('n experiments:',len(exps))
print('batch size:',bs)
print('batches per epoch:',n_batches)
imp=implementation('mlp_batchnorm',act) if batchnorm else implementation('mlp_no_ll_act',act)

loss=mk_cross_entropy(lambda w,x:imp(w,x,ll_act=True),eps=1e-4)
epochs,dlo=mk_epochs(imp,n_batches,bs,loss,X_train,Y_train,X_test,Y_test,
                     batchnorm=batchnorm,adapthresh_tol=False)
term_size=gts()
epochs_per_eval=1#5
all_benches=[]
ns=norms_states(states,[s['w'] for s in states])
for step in count():
  ts.get_timestep('start')
  states,l2_states,consts,recent_benches=epochs(states,consts,ke.emit_key(),epochs_per_eval)
  all_benches+=recent_benches
  print('Epoch',(1+step)*epochs_per_eval)

  print(show_unique_stats([{**s,**b,**c,'p_test':p_test,'shape':sh} for s,c,b,sh in\
                           zip(l2_states,consts,all_benches[-1],
                               ['->'.join([str(s) for s in sh]) for sh in shapes])],
                          trunc=True,by=['fpfn','fp_trn','fn_trn','lr'],
                          prec=4,term_size=term_size,ts=ts,flatten_pid=False),
          flush=True)
  ts.get_timestep('rep_states')
