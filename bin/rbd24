#!/usr/bin/env python
from pickle import load
from jax.numpy import inf,array,array_split,zeros
from numpy import array as nparr
from jax.lax import dot_general,scan
from jax.random import key,split
from jax import jit,grad
from jaxlib.xla_extension import XlaRuntimeError
from sys import path,stdin
from os import mkdir,environ

from os.path import dirname,abspath
from sys import path
path.append(dirname(dirname(abspath(__file__))))
from cal.thlay import init_ensemble,init_experiments,activations,f_to_str,\
                      save_ensemble,exp_to_str,fpfnp_lab,f,get_xy,report_progress,\
                      update_lrs,losses,evaluate_fp_fn,update_history,compute_U_V,\
                      update_weights,resnet,read_input_if_ready,fm,implementation,\
                      TimeStepper,U_V_scale,U_V_sm,U_V_l2,upd_adam,upd_grad,KeyEmitter,\
                      shuffle_xy,fp_fn_nl,fp_fn_ewma,report_epochs,plot_epochs,update_epochs

a,ke,ts,experiments=init_ensemble()

act=activations[a.activation] if a.activation else None
imp=implementation(a.implementation,act)
_dimp=value_and_grad(imp)
dimp=vmap(_dimp,in_axes=[None,0])
imp_bin=jit(lambda w,x:imp(w,x).flatten()>0)

#U_V=U_V_sm if a.softmax_U_V else U_V_l2#U_V_scale
#fp_fn=fp_fn_ewma if a.trad_avg else fp_fn_nl
def U_V(s,fp,fn):
  u=fp/s['target_fp']
  v=s['pp']*fn/s['target_fn']
  l=(u**2+v**2)**.5
  return u/,v/l

loss=losses[a.loss](act,imp)
gloss=jit(grad(loss))
emptarr=array([])
def all_step(states,targets,x,y):
  ret=[]
  for s in zip(states,targets):
    y_p=imp_bin(s['w'],x)
    fp_batch=(y_p&(~y)).mean()
    fn_batch=((~y_p)&y).mean()
    fp=(1-s['avg_rate'])*s['fp']+s['avg_rate']*fp_b
    fn=(1-s['avg_rate'])*s['fn']+s['avg_rate']*fn_b
    U,V=U_V(s,fp,fn)
    dw=gloss(w,x,y,U,V,e.eps,e.reg)
    w,ad_v,ad_m=upd_adam(w,ad_v,ad_m,dw,e.beta1,e.beta2,e.lr,e.eps)
    ret.append((w,ad_v,ad_m,fp,fn))
  return ret

_all_step=lambda vs,xy:all_step(*vs,*xy)

get targets=lambda e:e.target_fp,target_fn
get_state=lambda e:{'w':e.w_model,'ad_v':e.ad_v,'ad_m':e.ad_m,'fp':e.fp,'fn':e.fn}
set_state=lambda e,s:(e.w_model,e.ad_v,e.ad_m,e.fp,e.fn)=s

def _all_epoch(X,Y,states,hypers,target_fps,target_fns,n_batches,bs):
  X_b=X[:n_batches*bs].reshape((n_batches,bs,-1))
  Y_b=Y[:n_batches*bs].reshape((n_batches,bs))
  return scan(_all_step,zip(states,hypers,target_fps,target_fns),(X_b,Y_b))[0]
all_epoch=jit(_all_epoch,static_argnames=['n_batches','bs'])

def __all_predict(X,Y,states,n_batches,bs):
  X_b=X[:n_batches*bs].reshape((n_batches,bs,-1))
  Y_b=Y[:n_batches*bs].reshape((n_batches,bs))
  fs=[]
  n_rows=(n_batches*bs)
  for s in states:
    Y_p=[imp_bin(s[0],x) for x in X_b]
    fs.append((sum([(y_p&(~y)).sum() for y,y_p in zip(Y_b,Y_p)])/n_rows,
               sum([((~y_p)&y).sum() for y,y_p in zip(Y_b,Y_p)])/n_rows))
  return fs

_all_predict=jit(__all_predict,static_argnames=['n_batches','bs'])

split_dict={}
def all_predict(X,Y,states,n_batches,bs):
  _n_batches=n_batches
  _bs=bs
  n_splits=1
  try:
    n_batches,bs,n_splits=split_dict[(_n_batches,_bs)]
  except:
    split_dict[(_n_batches,_bs)]=[n_batches,bs,1]
  while True:
    split_len=n_batches*bs
    best_len=split_len*n_splits
    try:
      a_args=[(x,y,states,n_batches,bs)for x,y in\
              zip(X[:best_len].reshape((n_splits,split_len,-1)),
                  Y[:best_len].reshape((n_splits,split_len)))]
      res=[_all_predict(*args) for args in a_args]
      ret=[]
      fpfn=[[0,0] for i in range(len(states))]
      for re in res:
        for i,r in enumerate(re):
          fpfn[i][0]+=r[0]/n_splits
          fpfn[i][1]+=r[1]/n_splits
      return fpfn
    except XlaRuntimeError:
      print('Shrinking prediction batches and moving large'
            ' dataset from gpu to conserve memory')
      X=nparr(X)
      Y=nparr(Y)
      split_dict[(_n_batches,_bs)][0]*=2
      split_dict[(_n_batches,_bs)][1]//=4
      split_dict[(_n_batches,_bs)][2]*=2
      print('n_batches,bs,n_spltis:',*split_dict[(_n_batches,_bs)])
      if not split_dict[(_n_batches,_bs)][1]:
        raise Exception('Memory mess')

a.n_batches=a.n_rows_train//a.bs

experiment_states=[get_state(e) for e in experiments]
ts.get_timestep()
while True:
  ts.get_timestep('start')
  X,Y=shuffle_xy(ke.emit_key(),a.x_train,a.y_train)
  ts.get_timestep('shuffle')
  print('Starting epoch...')
  experiment_states=all_epoch(X,Y,experiment_states,a.n_batches,a.bs)
  a.epoch_num+=1
  ts.get_timestep('epoch')
  print('end epoch',a.epoch_num)
  for fs_test,fs_train,e in\
  zip(all_predict(a.x_test,a.y_test,experiment_states,1,a.n_rows_test),
      all_predict(a.x_train,a.y_train,experiment_states,1,a.n_rows_train),
      experiments):
    (e.fp_test,e.fn_test)=fs_test
    (e.fp_train,e.fn_train)=fs_train
  ts.get_timestep('predicting')
  [set_state(e,s) for e,s in zip(experiments,experiment_states)]
  ts.get_timestep('set_state')
  [update_epochs(e) for e in experiments]
  check_completed(experiments,experiment_states)
  if not a.n_active_experiments:
    print('All experiments complete, exiting')
    line='erx'
  else:
    line=''
  line+=read_input_if_ready()
  report_epochs(a,experiments,ts,line,imp,ke.emit_key(report=True))
  ts.get_timestep('reporting')
  save_ensemble(a,experiments,ke,ts)
  ts.get_timestep('saving')
