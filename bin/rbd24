#!/usr/bin/env python
from pickle import load
from jax.numpy import inf,array,array_split,zeros
from numpy import array as nparr
from jax.lax import dot_general,scan
from jax.random import key,split
from jax import jit,grad
from jaxlib.xla_extension import XlaRuntimeError
from sys import path,stdin
from os import mkdir,environ
#environ["XLA_PYTHON_CLIENT_PREALLOCATE"]="false"
#environ["XLA_PYTHON_CLIENT_MEM_FRACTION"]=".99"
#environ["XLA_PYTHON_CLIENT_ALLOCATOR"]="platform"

from os.path import dirname,abspath
from sys import path
path.append(dirname(dirname(abspath(__file__))))
from cal.thlay import init_ensemble,init_experiments,activations,f_to_str,\
                      save_ensemble,exp_to_str,fpfnp_lab,f,get_xy,report_progress,\
                      update_lrs,losses,evaluate_fp_fn,update_history,compute_U_V,\
                      update_weights,resnet,read_input_if_ready,fm,implementation,\
                      TimeStepper,U_V_scale,U_V_sm,upd_adam,upd_grad,KeyEmitter,\
                      shuffle_xy,fp_fn_nl,fp_fn_ewma,report_epochs,plot_epochs,update_epochs

a=init_ensemble()
try:
  mkdir(a.out_dir)
  new=True
except FileExistsError:
  print('Already seems to be something there... [O]verwrite, [L]oad or [A]bort?')
  ln=stdin.readline()[0].lower()
  if ln[0]=='l':
    new=False
  elif ln[0]=='o':
    new=True
    print('Overwriting any existing ensembles...')
  else:
    print('Abort!')
    exit()
if not new:
  try:
    with open(a.out_dir+'/ensemble.pkl','rb') as fd:
      print('Opening experiment ensemble',a.outf,'...')
      od=a.out_dir #Correct the actual directory if opened somewhere else
      a,experiments=load(fd)
      ke=KeyEmitter(a.seed,a.parent_keys)
      print(a.time_avgs)
      ts=TimeStepper(a.clock_avg_rate,a.time_avgs)
      a.out_dir=od
      new=False
      print('Restored',a.outf+'.pkl','from disk')
  except FileNotFoundError:
    print('No pkl in directory...')
    new=True
if new:
  print('Generating new experiments...')
  ke=KeyEmitter(a.seed)
  ts=TimeStepper(a.clock_avg_rate)
  experiments=init_experiments(a,ke)

act=activations[a.activation] if a.activation else None
imp=implementation(a.implementation,act)
imp_bin=jit(lambda w,x:imp(w,x).flatten()>0)

U_V=U_V_sm if a.softmax_U_V else U_V_scale
fp_fn=fp_fn_ewma if a.trad_avg else fp_fn_nl

loss=losses[a.loss](act,imp)
gloss=jit(grad(loss))
emptarr=array([])
def all_step(states,xy):
  x,y=xy
  ret=[]
  for e,s in zip(experiments,states):
    w,ad_v,ad_m,fp,fn=s
    y_p=imp_bin(w,x)
    fp_b=(y_p&(~y)).sum()/e.bs
    fn_b=((~y_p)&y).sum()/e.bs
    fp,fn=fp_fn_nl(e.bs,e.avg_rate,fp,fn,fp_b,fn_b)
    U,V=U_V_scale(fp,fn,e.target_fp,e.target_fn,e.pp)
    dw=gloss(w,x,y,U,V,e.eps,e.reg)
    w,ad_v,ad_m=upd_adam(w,ad_v,ad_m,dw,e.beta1,e.beta2,e.lr,e.eps)
    ret.append((w,ad_v,ad_m,fp,fn))
  return ret,0.

get_consts=lambda e:(e.lr,e.reg,e.bs,e.eps,e.pp,e.beta1,e.beta2,
                    e.avg_rate,e.target_fp,e.target_fn)
def set_state(e,s):
  #if not e.steps_to_target:
  (e.w_model,e.adam_V,e.adam_M,e.fp,e.fn)=s
get_state=lambda e:(e.w_model,e.adam_V,e.adam_M,e.fp,e.fn)

def check_completed(experiments,states):
  completed_experiments=[e for e in experiments if e.steps_to_target]
  #for i in [j for j,e in enumerate(experiments) if\
  #          e in completed_experiments]:
  #  states[i][0]=([0.*a for a in states[i][0][0]],
  #                [0.*b for b in states[i][0][1]])#force grad=0

def _all_epoch(X,Y,init_states,n_batches,bs):
  X_b=X[:n_batches*bs].reshape((n_batches,bs,-1))
  Y_b=Y[:n_batches*bs].reshape((n_batches,bs))
  return scan(all_step,init_states,(X_b,Y_b))[0]
all_epoch=jit(_all_epoch,static_argnames=['n_batches','bs'])

def __all_predict(X,Y,states,n_batches,bs):
  X_b=X[:n_batches*bs].reshape((n_batches,bs,-1))
  Y_b=Y[:n_batches*bs].reshape((n_batches,bs))
  fs=[]
  n_rows=(n_batches*bs)
  for s in states:
    Y_p=[imp_bin(s[0],x) for x in X_b]
    fs.append((sum([(y_p&(~y)).sum() for y,y_p in zip(Y_b,Y_p)])/n_rows,
               sum([((~y_p)&y).sum() for y,y_p in zip(Y_b,Y_p)])/n_rows))
  return fs

_all_predict=jit(__all_predict,static_argnames=['n_batches','bs'])

split_dict={}
def all_predict(X,Y,states,n_batches,bs):
  _n_batches=n_batches
  _bs=bs
  n_splits=1
  try:
    n_batches,bs,n_splits=split_dict[(_n_batches,_bs)]
  except:
    split_dict[(_n_batches,_bs)]=[n_batches,bs,1]
  while True:
    split_len=n_batches*bs
    best_len=split_len*n_splits
    try:
      a_args=[(x,y,states,n_batches,bs)for x,y in\
              zip(X[:best_len].reshape((n_splits,split_len,-1)),
                  Y[:best_len].reshape((n_splits,split_len)))]
      res=[_all_predict(*args) for args in a_args]
      ret=[]
      fpfn=[[0,0] for i in range(len(states))]
      for re in res:
        for i,r in enumerate(re):
          fpfn[i][0]+=r[0]/n_splits
          fpfn[i][1]+=r[1]/n_splits
      return fpfn
    except XlaRuntimeError:
      print('Shrinking prediction batches and moving large'
            ' dataset from gpu to conserve memory')
      X=nparr(X)
      Y=nparr(Y)
      split_dict[(_n_batches,_bs)][0]*=2
      split_dict[(_n_batches,_bs)][1]//=4
      split_dict[(_n_batches,_bs)][2]*=2
      print('n_batches,bs,n_spltis:',*split_dict[(_n_batches,_bs)])
      if not split_dict[(_n_batches,_bs)][1]:
        raise Exception('Memory mess')

a.n_batches=a.n_rows_train//a.bs

experiment_states=[get_state(e) for e in experiments]
ts.get_timestep()
while True:
  ts.get_timestep('start')
  X,Y=shuffle_xy(ke.emit_key(),a.x_train,a.y_train)
  ts.get_timestep('shuffle')
  print('Starting epoch...')
  experiment_states=all_epoch(X,Y,experiment_states,a.n_batches,a.bs)
  a.epoch_num+=1
  ts.get_timestep('epoch')
  print('end epoch',a.epoch_num)
  for fs_test,fs_train,e in\
  zip(all_predict(a.x_test,a.y_test,experiment_states,1,a.n_rows_test),
      all_predict(a.x_train,a.y_train,experiment_states,1,a.n_rows_train),
      experiments):
    (e.fp_test,e.fn_test)=fs_test
    (e.fp_train,e.fn_train)=fs_train
  ts.get_timestep('predicting')
  [set_state(e,s) for e,s in zip(experiments,experiment_states)]
  ts.get_timestep('set_state')
  [update_epochs(e) for e in experiments]
  check_completed(experiments,experiment_states)
  if not a.n_active_experiments:
    print('All experiments complete, exiting')
    line='erx'
  else:
    line=''
  line+=read_input_if_ready()
  report_epochs(a,experiments,ts,line,imp,ke.emit_key(report=True))
  ts.get_timestep('reporting')
  save_ensemble(a,experiments,ke,ts)
  ts.get_timestep('saving')
